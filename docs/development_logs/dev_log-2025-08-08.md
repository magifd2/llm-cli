# Development Log

This document records the detailed development history and key decisions made during the project.

## 2025-08-08 (Refactoring: Provider Package Separation)

- **Objective**: To resolve the growing technical debt in the `internal/llm` package by refactoring it into a modular, package-per-provider architecture.
- **Problem**: The `internal/llm` package had become a "Fat Package," containing the logic for all providers. This led to tight coupling, as seen when implementing `openai2`, which initially had incorrect dependencies on `openai`. This structure hindered maintainability and made future unit testing nearly impossible.
- **Solution**: 
    1.  **Architectural Shift**: Each provider (`ollama`, `bedrock`, `openai`, `openai2`, `vertexai`, `vertexai2`) was moved into its own self-contained package under `internal/llm/`.
    2.  **Enforcing Independence**: To ensure true modularity, providers were made fully self-contained. Shared code (like request/response structs or helper functions) was intentionally duplicated in each new package. This prioritizes independence over DRY principles for now, making each provider a standalone unit that can be modified or deleted without side effects.
    3.  **Central Interface**: The core `llm.Provider` interface and the new shared `llm.Message` struct were kept in the parent `internal/llm` package to provide a consistent contract for all providers.
    4.  **Manual Verification**: Due to the lack of unit tests for providers, each step of the refactoring was verified by building the application and performing manual tests against live backends (`LM Studio`, `Vertex AI`, `Bedrock`) to ensure no regressions were introduced.
- **Implementation Details**:
    - Created new directories: `internal/llm/ollama`, `internal/llm/bedrock`, etc.
    - Moved and refactored each provider's `.go` file into its new respective package.
    - Updated `cmd/prompt.go` to import and instantiate providers from their new locations.
    - Updated `DEVELOPMENT_PLAN.md` to reflect the new architecture and the unblocking of the testing strategy.
- **Outcome**: The project is now in a much healthier state. The clear separation of concerns makes the codebase easier to understand and maintain. Most importantly, this new structure removes the primary obstacle to implementing a robust unit testing strategy, paving the way for a more stable and reliable application.

## 2025-08-08 (Feature: Advanced Model Resolution for OpenAI-Compatible APIs)

- **Objective**: To significantly improve the user experience with OpenAI-compatible APIs (especially local servers like LM Studio) by implementing a flexible and robust model resolution mechanism.
- **Problem**: Users needed to manually update their profile's `model` setting every time they loaded a different model into their local server. This was cumbersome and inefficient.
- **Solution**: 
    1.  **New Provider (`openai2`)**: Introduced a new `openai2` provider to house the advanced logic without altering the standard `openai` provider.
    2.  **Prioritized Model List**: The `model` setting in a profile is now treated as a comma-separated, prioritized list (e.g., `"my-favorite-model,auto"`).
    3.  **Dynamic Resolution Logic**: A new `resolveModel` function was implemented. It fetches the list of available models from the server's `/v1/models` endpoint. It then iterates through the user's priority list, checking each entry against the available models. The first valid model found is used.
        - A **specific model name** is checked for its presence in the available list.
        - The keyword **`auto`** resolves to the first model in the available list.
- **Implementation Details**:
    - `internal/llm/openai2.go`: Created a new file for the `OpenAI2Provider`, containing the core logic for fetching and resolving models.
    - `internal/llm/openai.go`: Refactored by removing shared struct definitions (like `openAIRequest`) from `openai2.go` to prevent redeclaration errors, making `openai2.go` dependent on the common definitions in `openai.go`.
    - `cmd/prompt.go`: Added a `case` for `openai2` to the provider selection `switch`.
    - `README.md`: Updated extensively to document the new `openai2` provider, explaining the priority list mechanism with clear examples (`model1,auto`, `auto,fallback`, etc.).
- **Outcome**: This feature provides a powerful and flexible way for users to manage models. It minimizes the need for profile edits, accommodates various user workflows (e.g., "use this specific model if available, otherwise use any other"), and makes the tool significantly more convenient for local LLM experimentation.

## 2025-08-08 (Release: v0.0.11)

- **Objective**: Release the new `vertexai2` provider and other enhancements as version `v0.0.11`.
- **Process**:
    - Verified the new provider's functionality through manual testing.
    - Confirmed all automated tests pass (`make test`).
    - Confirmed no vulnerabilities are present (`make vulncheck`).
    - Updated `CHANGELOG.md` and `CHANGELOG.ja.md` for the `v0.0.11` release.
    - Created this development log entry to document the release.
    - The release will be finalized by committing the changes, tagging the release, and creating a GitHub Release.

## 2025-08-08 (Feature: Enhanced Vertex AI System Prompt Handling)

- **Objective**: To improve the handling of system prompts for the Google Cloud Vertex AI provider by implementing a more robust workaround for the SDK's lack of native system prompt support.
- **Problem**: The existing `vertexai` provider simulated system prompts by sending them as the first user message. While functional, this approach could sometimes lead to inconsistent model behavior.
- **Solution**: 
    1.  **New Provider (`vertexai2`)**: Introduced a new provider named `vertexai2` to house the enhanced implementation without affecting the existing, stable `vertexai` provider.
    2.  **History Priming**: The `vertexai2` provider now primes the conversation by injecting a two-turn history at the beginning of the chat session. It passes a `history` slice to `client.Chats.Create()` containing:
        - A `user` role message with the content of the system prompt.
        - A `model` role message with a simple acknowledgment (e.g., "OK.").
    3.  **SDK Analysis**: The development process involved significant trial and error due to misunderstandings of the `google.golang.org/genai` SDK. The final, correct implementation was achieved after analyzing the SDK's source code (`chats.go`, `types.go`) to correctly construct the `history` slice with `[]*genai.Content` and `[]*genai.Part` types.
- **Implementation Details**:
    - `internal/llm/vertexai2.go`: Created a new file for the `VertexAI2Provider`.
    - `cmd/prompt.go`: Added a `case` for `vertexai2` to the provider selection `switch`.
    - `internal/config/config.go`: Updated comments to include `vertexai2` as a valid provider option.
    - `README.md` & `README.ja.md`: Updated to explain the two Vertex AI providers and the benefits of the new `vertexai2` history priming method.
    - `CHANGELOG.md` & `CHANGELOG.ja.md`: Added an entry for the new feature.
- **Outcome**: The new `vertexai2` provider offers a more reliable method for setting the context for conversations with Vertex AI models, leading to more predictable and consistent outputs.