# Development Log

This document records the detailed development history and key decisions made during the project.

## 2025-08-08 (Feature: Advanced Model Resolution for OpenAI-Compatible APIs)

- **Objective**: To significantly improve the user experience with OpenAI-compatible APIs (especially local servers like LM Studio) by implementing a flexible and robust model resolution mechanism.
- **Problem**: Users needed to manually update their profile's `model` setting every time they loaded a different model into their local server. This was cumbersome and inefficient.
- **Solution**: 
    1.  **New Provider (`openai2`)**: Introduced a new `openai2` provider to house the advanced logic without altering the standard `openai` provider.
    2.  **Prioritized Model List**: The `model` setting in a profile is now treated as a comma-separated, prioritized list (e.g., `"my-favorite-model,auto"`).
    3.  **Dynamic Resolution Logic**: A new `resolveModel` function was implemented. It fetches the list of available models from the server's `/v1/models` endpoint. It then iterates through the user's priority list, checking each entry against the available models. The first valid model found is used.
        - A **specific model name** is checked for its presence in the available list.
        - The keyword **`auto`** resolves to the first model in the available list.
- **Implementation Details**:
    - `internal/llm/openai2.go`: Created a new file for the `OpenAI2Provider`, containing the core logic for fetching and resolving models.
    - `internal/llm/openai.go`: Refactored by removing shared struct definitions (like `openAIRequest`) from `openai2.go` to prevent redeclaration errors, making `openai2.go` dependent on the common definitions in `openai.go`.
    - `cmd/prompt.go`: Added a `case` for `openai2` to the provider selection `switch`.
    - `README.md`: Updated extensively to document the new `openai2` provider, explaining the priority list mechanism with clear examples (`model1,auto`, `auto,fallback`, etc.).
- **Outcome**: This feature provides a powerful and flexible way for users to manage models. It minimizes the need for profile edits, accommodates various user workflows (e.g., "use this specific model if available, otherwise use any other"), and makes the tool significantly more convenient for local LLM experimentation.

## 2025-08-08 (Release: v0.0.11)

- **Objective**: Release the new `vertexai2` provider and other enhancements as version `v0.0.11`.
- **Process**:
    - Verified the new provider's functionality through manual testing.
    - Confirmed all automated tests pass (`make test`).
    - Confirmed no vulnerabilities are present (`make vulncheck`).
    - Updated `CHANGELOG.md` and `CHANGELOG.ja.md` for the `v0.0.11` release.
    - Created this development log entry to document the release.
    - The release will be finalized by committing the changes, tagging the release, and creating a GitHub Release.

## 2025-08-08 (Feature: Enhanced Vertex AI System Prompt Handling)

- **Objective**: To improve the handling of system prompts for the Google Cloud Vertex AI provider by implementing a more robust workaround for the SDK's lack of native system prompt support.
- **Problem**: The existing `vertexai` provider simulated system prompts by sending them as the first user message. While functional, this approach could sometimes lead to inconsistent model behavior.
- **Solution**: 
    1.  **New Provider (`vertexai2`)**: Introduced a new provider named `vertexai2` to house the enhanced implementation without affecting the existing, stable `vertexai` provider.
    2.  **History Priming**: The `vertexai2` provider now primes the conversation by injecting a two-turn history at the beginning of the chat session. It passes a `history` slice to `client.Chats.Create()` containing:
        - A `user` role message with the content of the system prompt.
        - A `model` role message with a simple acknowledgment (e.g., "OK.").
    3.  **SDK Analysis**: The development process involved significant trial and error due to misunderstandings of the `google.golang.org/genai` SDK. The final, correct implementation was achieved after analyzing the SDK's source code (`chats.go`, `types.go`) to correctly construct the `history` slice with `[]*genai.Content` and `[]*genai.Part` types.
- **Implementation Details**:
    - `internal/llm/vertexai2.go`: Created a new file for the `VertexAI2Provider`.
    - `cmd/prompt.go`: Added a `case` for `vertexai2` to the provider selection `switch`.
    - `internal/config/config.go`: Updated comments to include `vertexai2` as a valid provider option.
    - `README.md` & `README.ja.md`: Updated to explain the two Vertex AI providers and the benefits of the new `vertexai2` history priming method.
    - `CHANGELOG.md` & `CHANGELOG.ja.md`: Added an entry for the new feature.
- **Outcome**: The new `vertexai2` provider offers a more reliable method for setting the context for conversations with Vertex AI models, leading to more predictable and consistent outputs.